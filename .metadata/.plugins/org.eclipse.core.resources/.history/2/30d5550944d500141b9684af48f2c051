/*Kati KyllÃ¶nen
 * 011539913
 * Big Data Framework
 * Exercise set 2
 * Exercise 2 Movies
 * 
 * Ratings:
 * UserID::MovieID::Rating::Timestamp
 * 
 * Movies:
 * MovieID::Title::Genres
 * 
 * Users:
 * UserID::Gender::Age::Occupation::Zip-code
 * 
 * Target:
 * RDD[(userID, gender, age, occupation, Set(movie, genre, rating))]
 * Be sure that you choose correct data types, e.g., IDs are Integers. 
 * 
 */

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf


object Movies {
  
  def main (args: Array[String]){
    val path = "/home/kati/foo/BDF/" // Path to files
    val conf = new SparkConf().setAppName("Movies").setMaster("local[2]")
    val sc = new SparkContext(conf)
    // read the files as RDDs and cache them
    val movies = sc.textFile(path+"movies.dat", 2)//.cache()
    val users = sc.textFile(path+"users.dat", 2)//.cache()
    val ratings = sc.textFile(path+"ratings.dat", 2)//.cache()
    
    
    val splittedUsersMap = users.map(line => line.split("::")).map(x => (x(0).toInt, (x(1), x(2).toInt, x(3).toInt)))
     println ("........-----splittedUsersMap" +splittedUsersMap.first() )
     //userId -> (gender, age, occupation) 
    
    val splittedMoviesMap = movies.map(line => line.split("::")).map(x => (x(0).toInt, (x(1), x(2)))) 
     println ("........-----splittedMoviesMap" +splittedMoviesMap.first() )
    // //movieId -> (Title, Genre)
     
    val splittedRatingsMap = ratings.map(line => line.split("::")).map(x => (x(1).toInt, (x(0).toInt, x(2).toInt, x(3).toInt))) 
     println ("........-----splittedRatingsMap" +splittedRatingsMap.first() )
     //movieId -> (UserId, Rating, Timestamp)
   
     val splittedRatingsMapByUser = ratings.map(line => line.split("::")).map(x => (x(0).toInt, (x(1).toInt, x(2).toInt, x(3).toInt))) 
     println ("........-----splittedRatingsMap" +splittedRatingsMap.first() )
     //userId -> (movieId, Rating, Timestamp)
    
     
     
     val moviesAndRatings = splittedMoviesMap.join(splittedRatingsMap)
     println (":::::::::::::::::::::::::"+moviesAndRatings.values.first())
     //(Title, Genre), (userID, Rating, Timestamp)
     //((Bonnie and Clyde (1967),Crime|Drama),(2,3,978298813))
     
     val moviesAndRatingsByUser = moviesAndRatings.values.map(x => (x._2._1, (x._1._1, x._1._2, x._2._2))).groupByKey
     println ("******************"+moviesAndRatingsByUser.first())
     //userId -> CompactBuffer(Title, Genre, Rating)
     //(4904,CompactBuffer((Mad Max (1979),Action|Sci-Fi,5), (Casablanca (1942),Drama|Romance|War,5)
     
     val res = splittedUsersMap.join(moviesAndRatingsByUser)
     println ("-.-.-.-.-.-.-.-.-.-.-."+res.first())
     //userID,(Gender, Age, Occupation)(Title, Genre, Rating)...
     //(4904,((M,50,15),CompactBuffer((Mad Max (1979),Action|Sci-Fi,5), (Casablanca (1942),Drama|Romance|War,5)... 
     
     println ("Adults have watched: "+numberOfWatchedMovies +" movies in total")
     topTen
     
     def numberOfWatchedMovies = {
      
      val adults = res.filter(x => x._2._1._2 >= 18)
      //println ("^^^^^^^^^^^^^^^^^^"+adults.first)
    
      val employedAdults = adults.filter(x => x._2._1._3 != 18 && x._2._1._3 != 4)
      //val tenAdults = employedAdults.take(10)
      //println(tenAdults.foreach(x => println(x)))
      val sum = employedAdults.map(x => x._2._2.size).reduce(_+_)
      sum
        
    }
   
        
    def topTen = {
      val onlyMovies = res.flatMap(x => x._2._2)
      val moviesByAge = res.map(x => (x._2._1._2, x._2._2)).groupByKey
      println (moviesByAge.first)
      
      
    }
         
         
     
      
  }

}