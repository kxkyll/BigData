package bdf.exercise2.katikyllonen

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD

object Solutions {
    val path = "/home/kati/foo/BDF/" // Path to files
      
  def main(args: Array[String]) {
    //val path = "/home/kati/foo/BDF/" // Path to files
    val conf = new SparkConf().setAppName("Movies").setMaster("local[2]")
    val sc = new SparkContext(conf)
   
    /* Run exercise2 code */
    val result = exercise2(sc)
    /* Run exercise3 code */
    val sum = exercise3(sc, result)
    /* Run exercise4 code */
    val topTenByAgeGroup = exercise4a(sc, result)
    exercise4b(topTenByAgeGroup)
    /* Run exercise5 code */
    exercise5(sc)
    /* Run exercise6 code */
    exercise6(sc)
    
    
  }
  
    //: RDD[(Int, Array[(String, Float)])]
  def exercise2 (sc: SparkContext): RDD[(Int,((String,Int,Int),Iterable[(String,String,Int)]))] ={
    // read the files as RDDs and cache them
    val movies = sc.textFile(path + "movies.dat", 2) //.cache()
    val users = sc.textFile(path + "users.dat", 2) //.cache()
    val ratings = sc.textFile(path + "ratings.dat", 2) //.cache()

    //Exercise 2 
    val splittedUsersMap = users.map(line => line.split("::")).map(x => (x(0).toInt, (x(1), x(2).toInt, x(3).toInt)))
    //println ("........-----splittedUsersMap" +splittedUsersMap.first() )
    //userId -> (gender, age, occupation) 

    val splittedMoviesMap = movies.map(line => line.split("::")).map(x => (x(0).toInt, (x(1), x(2))))
    //println ("........-----splittedMoviesMap" +splittedMoviesMap.first() )
    // //movieId -> (Title, Genre)

    val splittedRatingsMap = ratings.map(line => line.split("::")).map(x => (x(1).toInt, (x(0).toInt, x(2).toInt, x(3).toInt)))
    //println ("........-----splittedRatingsMap" +splittedRatingsMap.first() )
    //movieId -> (UserId, Rating, Timestamp)

    val splittedRatingsMapByUser = ratings.map(line => line.split("::")).map(x => (x(0).toInt, (x(1).toInt, x(2).toInt, x(3).toInt)))
    //println ("........-----splittedRatingsMap" +splittedRatingsMap.first() )
    //userId -> (movieId, Rating, Timestamp)

    val moviesAndRatings = splittedMoviesMap.join(splittedRatingsMap)
    //println (":::::::::::::::::::::::::"+moviesAndRatings.values.first())
    //(Title, Genre), (userID, Rating, Timestamp)
    //((Bonnie and Clyde (1967),Crime|Drama),(2,3,978298813))

    val moviesAndRatingsByUser = moviesAndRatings.values.map(x => (x._2._1, (x._1._1, x._1._2, x._2._2))).groupByKey
    //println ("******************"+moviesAndRatingsByUser.first())
    //userId -> CompactBuffer(Title, Genre, Rating)
    //(4904,CompactBuffer((Mad Max (1979),Action|Sci-Fi,5), (Casablanca (1942),Drama|Romance|War,5)

    //RDD[(Int,((String,Int,Int),Iterable[(String,String,Int)]))]
    
    
    val res = splittedUsersMap.join(moviesAndRatingsByUser)
    //println ("-.-.-.-.-.-.-.-.-.-.-."+res.first())
    //userID,(Gender, Age, Occupation)(Title, Genre, Rating)...
    //(4904,((M,50,15),CompactBuffer((Mad Max (1979),Action|Sci-Fi,5), (Casablanca (1942),Drama|Romance|War,5)... 
    return res
    
  }
  
  def exercise3 (sc: SparkContext, r: RDD[(Int,((String,Int,Int),Iterable[(String,String,Int)]))])= {
     // take adult age groups
      val adults = r.filter(x => x._2._1._2 >= 18)
      //println ("^^^^^^^^^^^^^^^^^^"+adults.first)
      // take only employed adulta
      val employedAdults = adults.filter(x => x._2._1._3 != 18 && x._2._1._3 != 4)
      // calculaate the sum of movies
      val sum = employedAdults.map(x => x._2._2.size).reduce(_ + _)
      println ("Employed adults have seen " +sum +" movies")
      sum
  }
  
  //rdd: RDD[(Int, ((String, Int, Int), Iterable[(String, String, Int)]))]): RDD[(Int, Array[(String, Float)])] = {
  def exercise4 (sc: SparkContext, rdd: RDD[(Int, ((String, Int, Int), Iterable[(String, String, Int)]))]): RDD[(Int, Array[(String, Float)])] = {
          //userID,(Gender, Age, Occupation)(Title, Genre, Rating)...
      val moviesByUser = rdd.map(x => (x._1, (x._2._1._2, x._2._2)))
      //userID, age, (title, genre, rating)
      val moviesByAge = moviesByUser.values
      //age, (title, genre, rating)
      val moviesFlat = moviesByAge.flatMap(x => x._2.map(y => ((x._1, y._1), y._3))).groupByKey
      //age, title, (rating)
      val averageRatings = moviesFlat.mapValues(x => (x.reduce(_ + _).toFloat / x.size))
      //(age, title), average
      val averageByAge = averageRatings.map(x => (x._1._1, (x._1._2, x._2))).groupByKey
      //age (title, average)
      val sortedTen = averageByAge.map(x => (x._1, x._2.toArray.sortBy(_._2)(Ordering[Float].reverse).take(10))).sortByKey()
      //println("::::::::::::::::::::::::::sortedTen.first " + sortedTen.first._2.foreach(y => println("Age: " + sortedTen.first._1 + " " + y._1 + " " + y._2)))

      return sortedTen

  }
  
  def exercise5 (sc: SparkContext){
    
  }
  
  def exercise6 (sc: SparkContext){
    
  }

}